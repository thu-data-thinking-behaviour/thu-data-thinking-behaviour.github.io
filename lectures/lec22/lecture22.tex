\documentclass[xetex,mathserif,serif,aspectratio=169]{beamer}

\input{../import.tex}
\usepackage[]{algorithm2e}
\usepackage{../kbordermatrix}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\vfill

{\fontsize{0.7cm}{0cm}\selectfont Lecture 22 \\\vspace{0.2cm}
Theory, Depth, Representation, Future}\\\vspace{0.5cm}
27 April 2016

\vspace{2cm}

\begin{minipage}{0.6\textwidth}
Taylor B. Arnold \\
Yale Statistics \\
STAT 365/665
\end{minipage}
\hfill
\begin{minipage}{0.3\textwidth}\raggedleft
\includegraphics[scale=0.3]{../yale-logo.png}
\end{minipage}%

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\yblue{\textbf{Notes}}

\begin{itemize}
\item Problem sets 7 and 8 will be returned by Thursday (maybe early Friday)
\item Problem set 9 is due next Monday
\item Blog posts coming soon!
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[height=8cm]{img/nlpScratch.jpg}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

The full citation:
\begin{quote}
Collobert, Ronan, Jason Weston, LÃ©on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. ``Natural language processing (almost) from scratch." The Journal of Machine Learning Research 12 (2011): 2493-2537.
\end{quote}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{flushright}
{\color{yaleblue}\sc\fontsize{1cm}{0cm}\selectfont Theory}
\end{flushright}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

One of the earliest theoretical results relating to neural
networks:
\begin{quote}
Barron, Andrew. ``Universal Approximation Bounds for Superpositions
of a Sigmoidal Function." IEEE Transactions on Information Theory,
Vol. 39, No.3, May 1993.
\end{quote}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

Consider functions of the form:
\begin{align*}
f_n(x) &= \sum_{k=1}^n c_k \cdot \sigma(a_k x + b_k) + c_0
\end{align*}
Which map $\mathbb{R}^d$ into $\mathbb{R}$.

This is a neural network with one hidden layer and a
single output. The parameters $a_k$ are the hidden weights,
the $b_k$ are the biases, $c_k$ are the output weights, and
$c_0$ is the output bias.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

In the paper it is shown that for a large class of
functions $f$, we can find a neural network such
that:
\begin{align*}
\int_{B_r} (f(x) - f_n(x))^2 dx &\leq C \times \frac{r^2}{n}
\end{align*}
For some constant $C > 0$.

This is a formal proof that even shallow neural networks are
universal approximators.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

Only recently have we seen theory addressing how well neural
networks can reconstruct generative models under noisy
observations. Two of the most well-known include:
\begin{quote}
Bengio, Yoshua, et al. ``Generalized denoising auto-encoders as generative models." Advances in Neural Information Processing Systems. 2013.
\end{quote}
And:
\begin{quote}
Alain, Guillaume, and Yoshua Bengio. ``What regularized auto-encoders learn from the data-generating distribution." The Journal of Machine Learning Research 15.1 (2014): 3563-3593.
\end{quote}
If you are interested in this line of work, I suggest setting
up an arXiv alert for people such as Yoshia Bengio, Guillaume Alain,
Razvan Pascanu, and Guido Mont\'{u}far.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[width=\textwidth]{img/valid1.pdf}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[width=\textwidth]{img/valid2.pdf}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[width=\textwidth]{img/valid3.pdf}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[width=\textwidth]{img/valid4.pdf}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[width=\textwidth]{img/valid5.pdf}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[width=\textwidth]{img/valid6.pdf}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[width=\textwidth]{img/valid7.pdf}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{flushright}
{\color{yaleblue}\sc\fontsize{1cm}{0cm}\selectfont Depth}
\end{flushright}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

So if shallow neural networks can represent arbitrary
functions, why have we been creating deeper and deeper
networks? A recent theoretical paper tries to explain
why deeper networks perform significantly better:
\begin{quote}
Montufar, Guido F., et al. ``On the number of linear regions of deep neural networks." Advances in neural information processing systems. 2014.
\end{quote}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[height=8cm]{img/linearUnits.jpg}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

A summary of one result from the paper: A dense
neural network with rectified linear activations having
$n_0$ input units and $L$ hidden layers of width $n \geq n_0$
can compute function that have:
\begin{align*}
\Omega \left( \left(\frac{n}{n_0} \right)^{(L-1) \cdot n_0} n^{n_0} \right)
\end{align*}
Number of linear regions. This shows that the expressibility
of the network grows exponentially with $L$ but only polynomially
with $n$.

So, deeper models approximate a larger class of functions with
fewer parameters.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[height=8cm]{img/representation.jpg}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[height=8cm]{img/vennDiagram.jpg}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{flushright}
{\color{yaleblue}\sc\fontsize{1cm}{0cm}\selectfont Representation}
\end{flushright}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \large

\begin{center}
\magenta{I. Problem Set 8 part 1} \\
\magenta{II. Problem Set 8, Part 2} \\
\magenta{III. Transfer Learning: IMDB Sentiment analysis}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

Neural networks have had amazingly successful results
learning things such as basic mathematical operations:
\begin{quote}
Franco, Leonardo, and Sergio A. Cannas. "Solving arithmetic problems using feed-forward neural networks." Neurocomputing 18.1 (1998): 61-79.
\end{quote}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \large

\begin{center}
\magenta{IV. Learning addition}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

There as also been work on using neural networks to
capture subjective features such as painting style:
\begin{quote}
Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge.
"A neural algorithm of artistic style." arXiv preprint
arXiv:1508.06576 (2015).
\end{quote}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[height=8cm]{img/styleTransfer.jpg}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\includegraphics[width=0.4\textwidth]{img/me.png}
\includegraphics[width=0.5\textwidth]{img/pollock.jpg}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[width=0.6\textwidth]{img/me_pollock.png}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\includegraphics[width=0.4\textwidth]{img/me.png}
\includegraphics[width=0.5\textwidth]{img/stary.jpg}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[width=0.6\textwidth]{img/me_stary.png}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{flushright}
{\color{yaleblue}\sc\fontsize{1cm}{0cm}\selectfont Future}
\end{flushright}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \large

Near-term popular areas of study in deep learning:
\begin{itemize}
\item compression of neural networks
\item consolidating the CNN tricks and tips; when will this
ever slow down or end?
\item deep residual neural networks
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \large

A robot at work:
\begin{center}
\magenta{\url{http://www.youtube.com/watch?v=2yRRNGr_4yY&t=1m0s}}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{
\setbeamertemplate{background}{\parbox[c][\paperheight][c]{\paperwidth}{\centering\includegraphics[width=0.7\paperwidth]{img/imageAnalysis1.pdf}}}
\begin{frame}[plain]
\end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{
\setbeamertemplate{background}{\parbox[c][\paperheight][c]{\paperwidth}{\centering\includegraphics[width=0.7\paperwidth]{img/imageAnalysis2.pdf}}}
\begin{frame}[plain]
\end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

To me, one of the more exciting papers on deep learning
produced in the past year:
\begin{quote}
Gregor, K., Danihelka, I., Graves, A., \& Wierstra, D. (2015). DRAW: A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623.
\end{quote}
It makes meaningful departures from prior methods and gets
more directly at the generative model. It is also closer to
our understanding of how visual processing happens in the
brain.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
\includegraphics[width=0.5\textwidth]{img/draw.jpg}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \large

Longer-term areas in deep learning:
\begin{itemize}
\item deep reinforcement learning
\item better architectures or training algorithms
\item (real) unsupervised learning
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\begin{center}
{\color{yaleblue}\sc\fontsize{2cm}{0cm}\selectfont Thanks!}
\end{center}

\end{frame}




\end{document}







