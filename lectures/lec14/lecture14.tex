\documentclass[xetex,mathserif,serif,aspectratio=169]{beamer}

\input{../import.tex}
\usepackage[]{algorithm2e}
\usepackage{../kbordermatrix}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\vfill

{\fontsize{0.7cm}{0cm}\selectfont Lecture 14 \\\vspace{0.2cm} Cost Functions
and Regularization}\\\vspace{0.5cm}
07 March 2016

\vspace{2cm}

\begin{minipage}{0.6\textwidth}
Taylor B. Arnold \\
Yale Statistics \\
STAT 365/665
\end{minipage}
\hfill
\begin{minipage}{0.3\textwidth}\raggedleft
\includegraphics[scale=0.3]{../yale-logo.png}
\end{minipage}%

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

Notes:
\begin{itemize}
\item Problem set 5 is due a week from today
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

Outline for this week:
\begin{itemize}
\item Monday
\begin{itemize}
\item walk through R reference implementation of back-propagation
\item address issues of slow learning
\item over-fitting / regularization
\item initializing weights
\end{itemize}
\item Wednesday
\begin{itemize}
\item quasi second order SGD
\item learning rate schedule
\item hyper-parameter selection heuristics
\end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\textbf{\yblue{Today's Notes}}

For today, rather than re-writing someone else's notes, I am
simply going to go through the notes on this page:
\begin{quote}
\url{http://neuralnetworksanddeeplearning.com/chap3.html}
\end{quote}
The primary reason for this is that there are some great
visualizations embedded on the page; also, as I have used the
exact same notation as presented here, there should be no great
confusion in doing so.

There are three summary slides here for reference purposes, which
summarize the main points I am covering today.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\textbf{\yblue{Cross-entropy and soft-max}}

Due to the shape of the sigmoid neuron, weights that are very far from
their optimal values learn slowly in a plain, vanilla network. Two ways
to fix this are to use the cross-entropy cost-function, defined as:
\begin{align*}
C &= - \sum_j \left[ y_j \log(a_j^L) + (1-y_j) \log(1 - a_j^L) \right]
\end{align*}
For a single sample, and similarly for an entire mini-batch.

Another common approach is to define what is termed a softmax layer.
The redefines the activations of the output later, $a^L$, as follows:
\begin{align*}
a^L_j &= \frac{e^{z_j^L}}{\sum_k e^{z_k^L}}
\end{align*}
This has the additional benefit that the last layer is easily interpreted
as a sequence of probabilities.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\textbf{\yblue{Regularization in Neural Networks}}

As the size of neural networks grow, the number of weights and biases can
quickly become quite large. State of the art neural networks today often
have billions of weight values. In order to avoid over-fitting, one common
approach is to add a penalty term to the cost function. Common choices are
the $\ell_2$-norm, given as:
\begin{align*}
C &= C_0 + \lambda \sum_i w_i^2
\end{align*}
Where $C_0$ is the unregularized cost, and the$\ell_1$-norm:
\begin{align*}
C &= C_0 + \lambda \sum_i |w_i|.
\end{align*}
The distinction between these is similar to the differences between lasso and
ridge regression.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\textbf{\yblue{Dropout}}

A very different approach to avoiding over-fitting is to use an approach called
\textit{dropout}. Here, the output of a randomly chosen subset of the neurons
are temporarily set to zero during the training of a given mini-batch. This makes
it so that the neurons cannot overly adapt to the output from prior layers as
these are not always present. It has enjoyed wide-spread adoption and massive
empirical evidence as to its usefulness.

\end{frame}

\end{document}











