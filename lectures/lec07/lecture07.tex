\documentclass[xetex,mathserif,serif,aspectratio=169]{beamer}

\input{../import.tex}
\usepackage[]{algorithm2e}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{} \oldB \small

\vfill

{\fontsize{0.7cm}{0cm}\selectfont Lecture 07 \\\vspace{0.2cm} Dimensionality Reduction with PCA}\\\vspace{0.5cm}
10 February 2016

\vspace{2cm}

\begin{minipage}{0.6\textwidth}
Taylor B. Arnold \\
Yale Statistics \\
STAT 365/665
\end{minipage}
\hfill
\begin{minipage}{0.3\textwidth}\raggedleft
\includegraphics[scale=0.3]{../yale-logo.png}
\end{minipage}%

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}  \oldB \small

As we have started to see, the curse of dimensionality stops
us from being able to fit arbitrarily complex models in high
dimensional spaces.

\blue{Additive models} try to avoid this by fixing the structure
of the learned models to limit interactions between the input variables.

\blue{Tree-based models} attempt to use the data itself to greedily
learn which interactions are actually important.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}  \oldB \small

Today we are going to look at another technique called
\blue{principal components} (PCs), or principal component
analysis (PCA), a specific example of dimensionality
reduction.

Like trees, these use the data to find lower dimensional structures
hidden in higher dimensional space. They differ from trees, however,
because principal components use \magenta{only the predictor variables} (not
the responses) and attempt to capture \magenta{global and linear structure},
rather than local ones.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}  \oldB \footnotesize

\yblue{\textbf{Motivating example}}

Say that we have a dataset of the following measurements from
a large set of human volunteers with the following variables:
\begin{itemize}
\item height
\item weight
\item waist size
\item shoe size
\item length of right arm
\item length of left arm
\item length of torso
\item pant inseam length
\item hat size
\item left hand ring size
\item right hand ring size
\end{itemize}
Technically we have $10$ variables, though most of the variation
in the dataset can probably be summarized by at most 2-3 summary variables.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}  \oldB \small

\yblue{\textbf{Motivating example, cont.}}

In decreasing order of variation, consider the following measurements
that can be derived from these $10$ variables
\begin{enumerate}
\item height
\item body mass index
\item ratio of torso length to total height
\end{enumerate}
Overall height captures a large amount of the variation in the total
dataset. Accounting separately for BMI, which in theory should be
relatively uncorrelated with overall height, captures much of the
next largest variation in the data. The final measurement attempts
to capture the remaining variation based on how height is distributed
over a given individuals frame.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}  \oldB \small

\yblue{\textbf{Motivating example, cont.}}

Conceptually, these values mimic what principal components attempt to
do: describe the maximum amount of variation in the data with a smaller
number of variables.

Each principal component, however, must be a linear function of the
input variables (so BMI would not be allowed). We also want them to
be defined mathematically rather than requiring us to hand construct
them for each dataset.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}  \oldB \small

\yblue{\textbf{Principal components}}

Formally, the principal components of the matrix $X$ are a linear
reparameterization $T=XW$ of the matrix $X$. The first column of
$T$ is the first principal component, the second column is the second
principal component, and so on.

Specifically, the matrix $W$ is defined uniquely by the following
conditions:
\begin{enumerate}
\item Each column of $T$ must be uncorrelated with the others;
specifically, W is an orthogonal matrix called the \textit{loadings}
\item The first column of $T$ has the largest variance of all
linear combinations of the columns of X, the second column has the
highest variance conditioned on being uncorrelated with the
first, and so forth.
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}  \oldB \small

\yblue{\textbf{Principal components, cont.}}

It can be shown that the matrix $W$ is equal to the eigenvectors
of the Gram matrix $X^tX$. From this relationship, there are many
results from numerical linear algebra that can be used to develop
theoretical results about principal components.

For the purposes of this course, however, we will be more concerned
with how they can actually be of use in data analysis, visualizations,
and predictive modeling. We will do that for the remainder of today's
class by applying them to several example datasets.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{}  \oldB \small

\yblue{\textbf{A look ahead}}

The main shortcoming of principal components are that they only
capture global linear structures in the data. This tends to be
a larger problem for prediction than it is for visualization.

Figuring out how to get non-linear extensions of principal
components is a wide open problem in statistic and machine learning.
Some avenues of research include:
\begin{itemize}
\item locally linear embedding
\item factor models
\item diffusion maps
\item mixture models
\end{itemize}
We will touch on some, though certainly not all, of these in the
upcoming weeks.

\end{frame}

\end{document}











