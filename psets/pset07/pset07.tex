\documentclass[12pt]{article}

\usepackage{fontspec}
\usepackage{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{listings}

\newcommand{\argmin}{\mathop{\mathrm{arg\,min}}}
\newcommand{\argmax}{\mathop{\mathrm{arg\,max}}}

\makeatletter
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%

\geometry{top=1in, bottom=1in, left=1in, right=1in, marginparsep=4pt, marginparwidth=1in}

\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancyplain}
\fancyhf{}
\cfoot{\thepage\ of \pageref{LastPage}}

\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

\usepackage{marginnote} % For margin years
\newcommand{\years}[1]{\marginnote{\scriptsize #1}} % New command for including margin years
\renewcommand*{\raggedleftmarginnote}{}
\setlength{\marginparsep}{-16pt} % Slightly increase the distance of the margin years from the content
\reversemarginpar

\setromanfont [Ligatures={Common}, Numbers={OldStyle}, Variant=01,
 BoldFont={LinLibertine_RB.otf},
 ItalicFont={LinLibertine_RI.otf},
 BoldItalicFont={LinLibertine_RBI.otf}
 ]{LinLibertine_R.otf}
%\setromanfont [Ligatures={Common}, Numbers={OldStyle}]{Hoefler Text}

%\usepackage[xetex, bookmarks, pdftitle={Taylor Arnold CV},pdfauthor={Taylor Arnold}]{hyperref}
%\hypersetup{linkcolor=blue,citecolor=blue,filecolor=black,urlcolor=MidnightBlue}

\usepackage{xunicode} % Allows generation of unicode characters from accented glyphs
\defaultfontfeatures{Mapping=tex-text}

\begin{document}

\begin{center}
{\bf Problem Set 07} \\
Data Mining and Machine Learning -- Spring 2016 \\
Due date: 2016-04-15 13:00
\end{center}

\medskip

All assignments must be uploaded to the assignments tab in ClassesV2
(notice that this is \textbf{not} the dropbox) by the date and time specified.
Make sure that you follow the instructions exactly as described.
You may discuss problem sets with others, but must write up your own
solutions. This means that you should have no need to look at other's
final written solutions.

You need to turn in all of your solutions as a zip compressed file, named
\texttt{netid\_pset07.zip}, with your actual netid filled in in all lower
case letters. This archive should contain the following two files:
\begin{itemize}
\item \texttt{pset07.pdf}
\item \texttt{pset07.py}
\end{itemize}
The python file will \textbf{not} be run or autograded, but is just for
showing your work for the assignment. The pdf file should contain results and
answers to the questions below.

\medskip

\textbf{General instructions}

For this problem set you will use python and keras library to fit models
to the CIFAR-10 dataset.
There is a starter code with some functions that you may find useful here:
\begin{quote}
\url{http://www.stat.yale.edu/~tba3/psets/pset07/pset07_starter.py}
\end{quote}
Throughout the assignment, unless otherwise noted, use the following
learning parameters:
\begin{itemize}
\item a validation split of $20\%$ of the training data
\item batch size of $32$
\item $25$ epochs, with early stopping using a patience of $2$
\item RMSprop learning algorithm, default settings
\item cost layer: `categorical\_crossentropy'
\item `relu' activation functions
\item dropout (tuning parameter of $0.5$) following every hidden layer
\item a final softmax layer
\item no weight regularization
\end{itemize}
You must upload your python script, but we will not be autograding it so
you do not need to worry about making it run on our machines. It is just
serves to show your work. \textit{Do not save this assignment to the last
minute! These models will take a while to run, depending on your hardware
some could take upwards of 10 minutes per epoch.}

\textbf{I. Convolution model kernel size}

Using the the two class version of the CIFAR-10 dataset, fit a model that
consists of a 2D-convolution and $2$-by-$2$ max pooling with `relu' activation
function, followed by a dense hidden layer with $512$
nodes (don't forget the `relu' activation and $0.5$ dropout), followed by the
output layer (complete with softmax). Run the convolution layer with $32$
filters using three kernel sizes: $1$, $3$, and $5$. How does the classification rate compare to
those from the last problem set? Present the classification rates and try
to explain why you see the pattern than you see. Explain in your own words
what the kernel of size $1$ is actually doing.

\textbf{II. Convolution model as autoencoder}

Repeat question I but use the model as an autoencoder (make sure to turn
off the softmax and use mean squared error as a cost function).
How does the pattern mimic and/or differ from that seen in question I?

\textbf{III. Freezing the features}

Using only a kernel of size $3$, take the two models from part I and II
and freeze the convolution layer (make sure you also copy the max-pooling
and activation function). Refit the top dense layer, but in both
cases do so on the $2$ class classification task. Notice that you already
did $2$ class classification for part I, so this just amount to re-training
the same model with the bottom frozen. For the model in part II, you'll
need to change the output layer to accommodate the new output dimensions
and the softmax needed for classification.
How do the testing and training rates change and compare to one another?
Give an explanation for why you might see such results.

\textbf{IV. Transfer learning}

Repeat question III, but now use the entire CIFAR-10 dataset (only the size
of the output layer needs to change in the structure of the models). To be clear, the
feature level is trained on only the two-class data, frozen, and then used as
an input to a single dense layer on the entire ten-class dataset. How does
this rate compare to the rates you had on problem set 5?

\textbf{V. Double convolution}

Using a kernel of size $3$ now use two successive convolution layers on the
2-class classification task. Present the classification rates and try
to explain why you see the pattern than you see. How does this compare to
the results in question I? Note that there should be an activation layer between
the two convolutions, but no max pooling or dropout until the end. How does
the result from this model compare to the result from using only one convolution?

\textbf{VI. Extensions}

Modify any of the models we constructed here to achieve the best classification
rates you can on the entire CIFAR-10 dataset. For example, change the learning
algorithm to plain vanilla SGD, add another dense layer, use regularization,
and/or increase the number of hidden nodes. Describe your model and give the
final classification rates.


\end{document}





