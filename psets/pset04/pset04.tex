\documentclass[12pt]{article}

\usepackage{fontspec}
\usepackage{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{listings}

\newcommand{\argmin}{\mathop{\mathrm{arg\,min}}}
\newcommand{\argmax}{\mathop{\mathrm{arg\,max}}}

\makeatletter
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%

\geometry{top=1in, bottom=1in, left=1in, right=1in, marginparsep=4pt, marginparwidth=1in}

\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancyplain}
\fancyhf{}
\cfoot{\thepage\ of \pageref{LastPage}}

\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

\usepackage{marginnote} % For margin years
\newcommand{\years}[1]{\marginnote{\scriptsize #1}} % New command for including margin years
\renewcommand*{\raggedleftmarginnote}{}
\setlength{\marginparsep}{-16pt} % Slightly increase the distance of the margin years from the content
\reversemarginpar

\setromanfont [Ligatures={Common}, Numbers={OldStyle}, Variant=01,
 BoldFont={LinLibertine_RB.otf},
 ItalicFont={LinLibertine_RI.otf},
 BoldItalicFont={LinLibertine_RBI.otf}
 ]{LinLibertine_R.otf}
%\setromanfont [Ligatures={Common}, Numbers={OldStyle}]{Hoefler Text}

%\usepackage[xetex, bookmarks, pdftitle={Taylor Arnold CV},pdfauthor={Taylor Arnold}]{hyperref}
%\hypersetup{linkcolor=blue,citecolor=blue,filecolor=black,urlcolor=MidnightBlue}

\usepackage{xunicode} % Allows generation of unicode characters from accented glyphs
\defaultfontfeatures{Mapping=tex-text}

\begin{document}

\begin{center}
{\bf Problem Set 04} \\
Data Mining and Machine Learning -- Spring 2016 \\
Due date: 2016-03-04 13:00
\end{center}

\medskip

All assignments must be uploaded to the assignments tab in ClassesV2
(notice that this is \textbf{not} the dropbox) by the date and time specified.
Make sure that you follow the instructions exactly as described.
This is a large class with a limited number of TA's and we will be
grading parts of the assignment using an automated grading engine;
you will lose points for things such as not naming files correctly.
You may discuss problem sets with others, but must write up your own
solutions. This means that you should have no need to look at other's
final written solutions.

You need to turn in all of your solutions as a zip compressed file, named
\texttt{netid\_pset04.zip}, with your actual netid filled in in all lower
case letters. This archive should contain just one file:
\begin{itemize}
\item \texttt{pset04.py} or \texttt{pset04.R}
\end{itemize}
The contents of this file is described below.

\medskip

\textbf{SVM Implementation}

The task for this assignment is to write two implementations to calculate
a linear support vector machine given a set of training data. In both cases
you do not need to support any kernels, nor do you need to include a bias
term (i.e., we are using a no-intercept model).

The first implementation uses the dual form of the support vector machine
equations. Specifically, you need to solve the optimization problem:
\begin{align*}
\max_\alpha& \quad \left\{ \sum_i \alpha_i - \frac{1}{2} \cdot \sum_{i'} \sum_i \alpha_i \alpha_{i'} y_i y_{i'}
x_i^t x_{i'} \right\} \\
&\text{s.t.} \quad 0 \leq \alpha_i \leq C.
\end{align*}
Which can be re-written as:
\begin{align*}
\max_\alpha&  \quad \left\{ 1_n^t \alpha_i - \alpha^t K \alpha \right\} \\
&\text{s.t.} \quad 0 \leq \alpha_i \leq C.
\end{align*}
For a suitable matrix $K$. The first implementation should feed this equation
into a general purpose library intended for solving constrained optimization
problems. (Hint: this should be fairly easy once you find a suitable
library; suggestions are given in the sample code).

The second task requires you to implement the basic Pegasos algorithm
from the following paper:
\begin{quote}
Shalev-Shwartz, Shai, Yoram Singer, and Nathan Srebro. ``Pegasos: Primal Estimated
sub-GrAdient SOlver for SVM.'' In: \textit{Proceedings of the 24th International
Conference on Machine Learning}, Corvallis, OR, 2007.
\end{quote}
You can download a copy of the paper here:
\begin{quote}
\url{http://www.ee.oulu.fi/research/imag/courses/Vedaldi/ShalevSiSr07.pdf}
\end{quote}
Specifically, you need to implement the algorithm in Figure 1 of the paper.

The function names and formats of these two required functions are
described in the starter code and test scripts on the course website:
\begin{quote}
\url{http://www.stat.yale.edu/~tba3/stat665/psets/pset04/pset04_start.py} \\
\url{http://www.stat.yale.edu/~tba3/stat665/psets/pset04/pset04_start.R} \\
\url{http://www.stat.yale.edu/~tba3/stat665/psets/pset04/testScript04.R} \\
\url{http://www.stat.yale.edu/~tba3/stat665/psets/pset04/testScript04.py}
\end{quote}
The test scripts serve to make sure that the \textbf{format} of your submission
is correct; it is up to you to build tests to see if the actual results make
sense. For example, compare your results to standard implementations of support
vector machines. Of particular note is the fact that you must return the vector
$\beta$, not the weights $\alpha$.


\end{document}





