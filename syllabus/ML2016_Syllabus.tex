\documentclass[12pt]{article}

\usepackage{fontspec}
\usepackage{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{hyperref}

\geometry{top=1in, bottom=1in, left=1in, right=1in, marginparsep=4pt, marginparwidth=1in}

\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancyplain}
\fancyhf{}
\cfoot{\thepage\ of \pageref{LastPage}}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}

\usepackage{marginnote} % For margin years
\newcommand{\years}[1]{\marginnote{\scriptsize #1}} % New command for including margin years
\renewcommand*{\raggedleftmarginnote}{}
\setlength{\marginparsep}{-16pt} % Slightly increase the distance of the margin years from the content
\reversemarginpar

\setromanfont [Ligatures={Common}, Numbers={OldStyle}, Variant=01,
 BoldFont={LinLibertine_RB.otf},
 ItalicFont={LinLibertine_RI.otf},
 BoldItalicFont={LinLibertine_RBI.otf}
 ]{LinLibertine_R.otf}
%\setromanfont [Ligatures={Common}, Numbers={OldStyle}]{Hoefler Text}

%\usepackage[xetex, bookmarks, pdftitle={Taylor Arnold CV},pdfauthor={Taylor Arnold}]{hyperref}
%\hypersetup{linkcolor=blue,citecolor=blue,filecolor=black,urlcolor=MidnightBlue}

\usepackage{xunicode} % Allows generation of unicode characters from accented glyphs
\defaultfontfeatures{Mapping=tex-text}

\begin{document}

\begin{center}
{\bf Data Mining and Machine Learning: STAT 365 / STAT 665} \\
Spring 2016 \quad Monday, Wednesdays 14:30 - 15:45 \quad DL 220
\end{center}

\bigskip

\noindent
\begin{tabular}{ l l }
{\bf Instructor:} &  {\bf Taylor Arnold} \\
E-mail: & \href{mailto:taylor.arnold@yale.edu}{taylor.arnold@yale.edu} \\
Office: & 24 Hillhouse, Rm 206 \\
Office Hours: & tbd \\
Teaching Assistants: & Yu Lu, Jason Klusowski \\
TA Hours: & tbd \\
Website: & \url{http://www.stat.yale.edu/~tba3/stat665/}
\end{tabular}

\vspace{0.5cm}

{\bf Course Description:} \\

Machine learning is an incredibly diverse field that sits at
the intersection of computer science and applied statistics.
This course will concentrate on the applied aspects of machine
learning, centered around supervised classification problems.
We will briefly cover regularized linear models and support
vector machines before spending the majority of the semester
on neural networks. Applications to problems in natural
language processing and computer vision will serve as
motivating examples.

\vspace{0.2cm}

{\bf Prerequisites:}
\begin{itemize}\setlength\itemsep{0em}
\item Proficient in R
\item Introductory statistical theory
\item Exposure to applied data analysis
\end{itemize}

\vspace{0.2cm}

{\bf References:}
\begin{itemize}\setlength\itemsep{0em}
\item Ian Goodfellow, Aaron Courville and Yoshua Bengio. \textit{Deep Learning}. Book in preparation for MIT Press. \url{http://www.deeplearningbook.org/}.
\item Jerome Friedman, Trevor Hastie and Robert Tibshirani. \textit{The Elements of Statistical Learning}. Springer, Berlin: Springer Series in Statistics, 2011.
\item Cosma Rohilla Shalizi. \textit{Advanced Data Analysis from an Elementary Point of View}. Book in preparation. \url{http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/}.
\item Stephen Boyd and Lieven Vandenberghe. \textit{Convex Optimization}. Cambridge University Press, 2004.
\end{itemize}

\vspace{0.2cm}

{\bf Problem Sets:} \\

There will be approximately $9$ problem sets assigned throughout the
semester, due on Thursdays. These will consist of both building custom
implementations of machine learning algorithms, as well as applying
established libraries to machine learning problems.
You should expect to become comfortable working simultaneously in a
number of programming languages. All submissions will be made
electronically on the ClassesV2 site.

\newpage

{\bf Grading:} \\

Course grades will be determined based on scores from the problem
sets. I want to make the grading extremely transparent, so these
will all be graded on an 10 point scale (with the possibility of
up to one additional point for truly exceptional work or extra
credit questions). The final grade will be calculated by dropping
the lowest grade, rounding the average of remainder to the nearest
integer and reading off of the following table:
\begin{center}
\begin{tabular}{c || l | c}
Numeric Score & \multicolumn{2}{| l}{Final Grade} \\
\hline \hline
10 & A  & H \\
9  & A- & H \\
8  & B+ & HP \\
7  & B  & HP \\
6  & B- & HP \\
5  & C+ & P \\
4  & C  & P \\
3  & C- & P \\
2  & D  & F \\
1  & F  & F \\
0  & F  & F
\end{tabular}
\end{center}

Because I will be dropping the lowest score, we will only
accept late assignments in the event of exceptional
circumstances (such as extended absences or family emergencies).
In particular, Undergraduates enrolled in STAT 365 must submit a
Dean's excuse for any late submission.

\vspace{0.5cm}

{\bf Tentative Schedule:}
\begin{itemize}\setlength\itemsep{0em}
\item 2016-01-20: \, Course introduction
\item 2016-01-22: \, Linear classification methods I (EoSL 3 \& 4)
\item 2016-01-25: \, Linear classification methods II (EoSL 3 \& 4)
\item 2016-01-27: \, Random forests and gradient boosting (EoSL 10 \& 15)
\item 2016-02-01: \, Support vector machines I (EoSL 12)
\item 2016-02-03: \, Support vector machines II (EoSL 12)
\item 2016-02-08: \, Introduction to Neural Networks I (DL 6.1-6.2)
\item 2016-02-10: \, Introduction to Neural Networks II (DL 6.3-6.4)
\item 2016-02-15: \, Back-propagation (DL 6.4)
\item 2016-02-17: \, Gradient Descent  (DL 8.1-8.3)
\item 2016-02-22: \, Adaptive Learning (DL 8.4)
\item 2016-02-24: \, Introduction to Theano
\item 2016-02-29: \, Computer Vision (DL 12.1)
\item 2016-03-02: \, Convolution Networks (DL 9.1-9.2)
\item 2016-03-07: \, Pooling in CNNs (DL 9.4)
\item 2016-03-09: \, Unsupervised CNNs and Transfer Learning (DL 9.8)
\item 2016-03-28: \, ILSVRC 2015 \& MS COCO - Object Detection
\item 2016-03-30: \, ILSVRC 2015 \& MS COCO - Object Localization
\item 2016-04-04: \, Deep Residual Learning (DL 20)
\item 2016-04-06: \, Moving Images
\item 2016-04-11: \, Natural Language Processing (DL 12.4)
\item 2016-04-13: \, Word Embeddings
\item 2016-04-18: \, Recurrent Neural Networks (DL 10)
\item 2016-04-20: \, Dependency Parsers I
\item 2016-04-25: \, Dependency Parsers II
\item 2016-04-27: \, Natural Language Inference
\end{itemize}

{\bf Tentative Problem Sets:}
\begin{itemize}\setlength\itemsep{0em}
\item 2015-02-04: \, Linear models and tree-based methods
\item 2015-02-11: \, Support vector machines
\item 2015-02-18: \, Intro to neural networks
\item 2015-02-25: \, Neural network optimization
\item 2015-03-10: \, Neural networks with Theano
\item 2015-03-31: \, Convolution networks and computer vision
\item 2015-04-07: \, Applied problems in computer vision
\item 2015-04-21: \, Recurrent neural networks
\item 2015-04-28: \, Applied problems in natural language processing
\end{itemize}

\vspace{0.5cm}

{\bf ML Datasets of interest:}

\begin{itemize}\setlength\itemsep{0em}
\item Taxi Data: \url{http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml}
\item Million Song Dataset: \url{http://labrosa.ee.columbia.edu/millionsong/}
\item MNIST: \url{http://yann.lecun.com/exdb/mnist/}
\item CIFAR-10/CIFAR-100: \url{https://www.cs.toronto.edu/~kriz/cifar.html}
\item The Street View House Numbers (SVHN) Dataset: \url{http://ufldl.stanford.edu/housenumbers/}
\item ILSVRC: \url{http://image-net.org/challenges/LSVRC/2015/}
\item Microsoft Common Images in Context (MS COCO): \url{http://mscoco.org/dataset/}
\item Stanford Natural Language Inference: \url{http://nlp.stanford.edu/projects/snli/}
\item TIMIT Continuous Speech Corpus: \url{http://catalog.ldc.upenn.edu/LDC93S1}
\end{itemize}

{\bf Selection of neural network software:}

\begin{itemize}\setlength\itemsep{0em}
\item torch: \url{http://torch.ch/}
\item Caffe: \url{http://caffe.berkeleyvision.org/}
\item theano: \url{http://deeplearning.net/software/theano/}
\item keras: \url{https://github.com/fchollet/keras}
\item blocks: \url{http://blocks.readthedocs.org/en/latest/}
\item Lasagne: \url{https://lasagne.readthedocs.org/en/latest/}
\item tensorflow: \url{https://github.com/tensorflow/tensorflow}
\item CNTK: \url{https://cntk.codeplex.com/}
\item deeplearning4j: \url{http://deeplearning4j.org/}
\end{itemize}

\vspace{0.2cm}

{\bf Selection of conference proceedings in machine learning:}

\begin{itemize}\setlength\itemsep{0em}
\item NIPS: \url{http://papers.nips.cc/}
\item ICML: \url{http://www.machinelearning.org/icml.html}
\item ICDM: \url{http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000179}
\item SIGKDD: \url{http://dl.acm.org/citation.cfm?id=2783258}
\item JMLR: \url{http://jmlr.csail.mit.edu/proceedings/}
\item AAAI: \url{http://www.aaai.org/Library/conferences-library.php}
\item ICCV, WACV, CVPR (vision): \url{http://pamitc.org/}
\item CoNLL (nlp): \url{http://ifarm.nl/signll/conll/}
\end{itemize}


\end{document}





